{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "a\n",
      "sentence\n",
      ".\n",
      "World\n",
      "World!\n",
      "Index:    [0, 1, 2, 3, 4, 5, 6]\n",
      "Text:     ['It', 'costs', '$', '5', '.', ' ', 'het422']\n",
      "is_alpha  [True, True, False, False, False, False, False]\n",
      "is_digit  [False, False, False, True, False, False, False]\n",
      "is_punct  [False, False, False, False, True, False, False]\n"
     ]
    }
   ],
   "source": [
    "# import english language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# create a nlp object\n",
    "nlp = English()\n",
    "\n",
    "# process a string of text with nlp object\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "    \n",
    "doc = nlp(\"Hello World!\")\n",
    "\n",
    "# index into the doc to get a single token\n",
    "token = doc[1]\n",
    "\n",
    "# get the token text via the .text attribute\n",
    "print(token.text)\n",
    "\n",
    "# a slice from the Doc is a Span object\n",
    "span = doc[1:3]\n",
    "print(span.text)\n",
    "\n",
    "# lexical attrubutes\n",
    "doc = nlp(\"It costs $5.  het422\")\n",
    "\n",
    "print(\"Index:   \", [token.i for token in doc])\n",
    "print(\"Text:    \", [token.text for token in doc])\n",
    "print(\"is_alpha \", [token.is_alpha for token in doc])\n",
    "print(\"is_digit \", [token.is_digit for token in doc])\n",
    "print(\"is_punct \", [token.is_punct for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Getting Started</h3>\n",
    "<p>1. Import the English class from spacy.lang.en and create the nlp object.</p>\n",
    "<p>2. Create a doc and print its text.<p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = English()\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Document, spans, tokens</h3>\n",
    "<h6>Step 1</h6>\n",
    "<p>1 Import the English language class and create the nlp object.</p>\n",
    "<p>2 Process the text and instantiate a Doc object in the variable doc.</p>\n",
    "<p>3Select the first token of the Doc and print its text.</p>\n",
    "\n",
    "<h6>Step 2</h6>\n",
    "<p>Create a slice of the Doc for the tokens “tree kangaroos” and “tree kangaroos and narwhals”.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree kangaroos\n",
      "tree kangaroos and narwhals\n"
     ]
    }
   ],
   "source": [
    "# Import the English language class and create the nlp object\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos\"\n",
    "tree_kangaroos = doc[2: 4]\n",
    "print(tree_kangaroos.text)\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
    "tree_kangaroos_and_narwhals = doc[2: 6]\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lexical attributes</h3>\n",
    "\n",
    "<p> 1 Use the like_num token attribute to check whether a token in the doc resembles a number.</p>\n",
    "<p> 2 Get the token following the current token in the document. The index of the next token in the doc is token.i + 1.</p>\n",
    "<p> 3 Check whether the next token’s text attribute is a percent sign ”%“.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n",
      "\n",
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n",
      "\n",
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model packages\n",
    "# python -m spacy download en_core_web_sm\n",
    "# It has:-\n",
    "#         Binary Weights\n",
    "#         Vocabulary\n",
    "#         Meta information(language, pipeline)\n",
    "\n",
    "# predicting part-of-speech tags\n",
    "import spacy\n",
    "\n",
    "# Load the small english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# process a text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# iterate over tokens\n",
    "for token in doc:\n",
    "    # print the text and predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)\n",
    "print()\n",
    "\n",
    "# predicting syntactic dependencies\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "print()\n",
    "\n",
    "# predicting named entities\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "print()\n",
    "\n",
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading models</h3>\n",
    "<p>1 Use spacy.load to load the small English model \"en_core_web_sm\".</p>\n",
    "<p>2 Process the text and print the document text.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the \"en_core_web_sm\" model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>predicting linguistic annotations</h3>\n",
    "<p>1 Process the text with the nlp object and create a doc.</p>\n",
    "<p>2 For each token, print the token text, the token’s .pos_ (part-of-speech tag) and the token’s .dep_ (dependency label).</p>\n",
    "<p>3 Iterate over the doc.ents and print the entity text and label_ attribute.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It          PRON      nsubj     \n",
      "’s          VERB      punct     \n",
      "official    NOUN      ccomp     \n",
      ":           PUNCT     punct     \n",
      "Apple       PROPN     nsubj     \n",
      "is          AUX       ROOT      \n",
      "the         DET       det       \n",
      "first       ADJ       amod      \n",
      "U.S.        PROPN     nmod      \n",
      "public      ADJ       amod      \n",
      "company     NOUN      attr      \n",
      "to          PART      aux       \n",
      "reach       VERB      relcl     \n",
      "a           DET       det       \n",
      "$           SYM       quantmod  \n",
      "1           NUM       compound  \n",
      "trillion    NUM       nummod    \n",
      "market      NOUN      compound  \n",
      "value       NOUN      dobj      \n",
      "\n",
      "Apple ORG\n",
      "first ORDINAL\n",
      "U.S. GPE\n",
      "$1 trillion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")\n",
    "print()\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Predicting named entites in context</h3>\n",
    "<p>1 Process the text with the nlp object.</p>\n",
    "<p>2 Iterate over the entities and print the entity text and label.</p>\n",
    "<p>3 Looks like the model didn’t predict “iPhone X”. Create a span for those tokens manually.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Missing entity: iPhone X\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Upcoming iPhone X release date leaked as Apple reveals pre-orders\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Get the span for \"iPhone X\"\n",
    "iphone_x = doc[1:3]\n",
    "\n",
    "# Print the span text\n",
    "print(\"Missing entity:\", iphone_x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n",
      "\n",
      "2018 FIFA World Cup:\n",
      "\n",
      "loved dogs\n",
      "love cats\n",
      "\n",
      "bought a smartphone\n",
      "buying apps\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Match patterns\n",
    "# List of dictionaries, one per token\n",
    "# Match exact token texts\n",
    "# [{\"TEXT\":\"iPhone\"}, {\"TEXT\":\"X\"}]\n",
    "\n",
    "# Match lexical attributes\n",
    "# [{\"LOWER\": \"iphone\"}, {\"LOWER\":\"x\"}]\n",
    "\n",
    "# Match any token attributes\n",
    "# [{\"LEMMA\"}: \"buy\", {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# using matcher (1)\n",
    "import spacy\n",
    "\n",
    "# import the matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# load a model and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Upcoming iPhone X release date leaked as Apple reveals pre-orders\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# initialize the matcher with shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", None, pattern)\n",
    "\n",
    "# Process the text\n",
    "matches = matcher(doc)\n",
    "\n",
    "# iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # get the matched span\n",
    "    matched_span = doc[start: end]\n",
    "    print(matched_span.text)\n",
    "print()\n",
    "\n",
    "# using matcher (2)\n",
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "matcher.add(\"FIFA_PATTERN\", None, pattern)\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "# Process the text\n",
    "matches = matcher(doc)\n",
    "\n",
    "# iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # get the matched span\n",
    "    matched_span = doc[start: end]\n",
    "    print(matched_span.text)\n",
    "print()\n",
    "\n",
    "# using matcher (3)\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "matcher.add(\"FIFA_PATTERN\", None, pattern)\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "# Process the text\n",
    "matches = matcher(doc)\n",
    "\n",
    "# iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # get the matched span\n",
    "    matched_span = doc[start: end]\n",
    "    print(matched_span.text)\n",
    "print()\n",
    "\n",
    "\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "matcher.add(\"FIFA_PATTERN\", None, pattern)\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "# Process the text\n",
    "matches = matcher(doc)\n",
    "\n",
    "# iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # get the matched span\n",
    "    matched_span = doc[start: end]\n",
    "    print(matched_span.text)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
